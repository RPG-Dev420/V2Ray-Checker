#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
V2Ray Config Automation System
Ø³ÛŒØ³ØªÙ… Ø§ØªÙˆÙ…Ø§Ø³ÛŒÙˆÙ† Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§
"""

import schedule
import time
import asyncio
import logging
import os
import json
from datetime import datetime
from config_collector import V2RayCollector

# ØªÙ†Ø¸ÛŒÙ… Ù„Ø§Ú¯
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('automation.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class AutomationManager:
    """Ù…Ø¯ÛŒØ±ÛŒØª Ø§ØªÙˆÙ…Ø§Ø³ÛŒÙˆÙ† Ø³ÛŒØ³ØªÙ… Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§"""

    def __init__(self):
        self.collector = V2RayCollector()
        self.is_running = False
        self.stats = {
            'total_runs': 0,
            'successful_runs': 0,
            'failed_runs': 0,
            'last_run': None,
            'last_successful_run': None
        }

        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¢Ù…Ø§Ø± Ù‚Ø¨Ù„ÛŒ
        self.load_stats()

    def load_stats(self):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¢Ù…Ø§Ø± Ø§Ø² ÙØ§ÛŒÙ„"""
        try:
            if os.path.exists('automation_stats.json'):
                with open('automation_stats.json', 'r', encoding='utf-8') as f:
                    self.stats = json.load(f)
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¢Ù…Ø§Ø±: {e}")

    def save_stats(self):
        """Ø°Ø®ÛŒØ±Ù‡ Ø¢Ù…Ø§Ø± Ø¯Ø± ÙØ§ÛŒÙ„"""
        try:
            with open('automation_stats.json', 'w', encoding='utf-8') as f:
                json.dump(self.stats, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ Ø¢Ù…Ø§Ø±: {e}")

    async def run_collection_job(self):
        """Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ø± Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§"""
        logger.info("ğŸ”„ Ø´Ø±ÙˆØ¹ Ú©Ø§Ø± Ø®ÙˆØ¯Ú©Ø§Ø± Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§...")

        self.stats['total_runs'] += 1
        self.stats['last_run'] = datetime.now().isoformat()

        try:
            # Ø§Ø¬Ø±Ø§ÛŒ Ø³ÛŒÚ©Ù„ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ
            subscription_files = await self.collector.run_collection_cycle()

            # ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´
            report = self.collector.generate_report()

            # Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´ Ø¨Ø§ timestamp
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            report_filename = f'subscriptions/report_{timestamp}.json'

            os.makedirs('subscriptions', exist_ok=True)
            with open(report_filename, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)

            # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¢Ù…Ø§Ø± Ù…ÙˆÙÙ‚ÛŒØª
            self.stats['successful_runs'] += 1
            self.stats['last_successful_run'] = datetime.now().isoformat()

            logger.info(
                f"âœ… Ú©Ø§Ø± Ø®ÙˆØ¯Ú©Ø§Ø± Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯ - {report['working_configs']} Ú©Ø§Ù†ÙÛŒÚ¯ Ø³Ø§Ù„Ù…")

            # Ø°Ø®ÛŒØ±Ù‡ Ø¢Ù…Ø§Ø±
            self.save_stats()

            return True

        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ú©Ø§Ø± Ø®ÙˆØ¯Ú©Ø§Ø±: {e}")
            self.stats['failed_runs'] += 1
            self.save_stats()
            return False

    def setup_schedule(self):
        """ØªÙ†Ø¸ÛŒÙ… Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ Ú©Ø§Ø±Ù‡Ø§"""

        # Ø§Ø¬Ø±Ø§ÛŒ Ù‡Ø± 30 Ø¯Ù‚ÛŒÙ‚Ù‡
        schedule.every(30).minutes.do(self.run_scheduled_job)

        # Ø§Ø¬Ø±Ø§ÛŒ Ù‡Ø± Ø³Ø§Ø¹Øª
        schedule.every().hour.do(self.run_health_check)

        # Ø§Ø¬Ø±Ø§ÛŒ Ø±ÙˆØ²Ø§Ù†Ù‡ Ø¯Ø± Ø³Ø§Ø¹Øª 2 ØµØ¨Ø­ Ø¨Ø±Ø§ÛŒ ØªÙ…ÛŒØ²Ú©Ø§Ø±ÛŒ
        schedule.every().day.at("02:00").do(self.cleanup_old_files)

        # Ø§Ø¬Ø±Ø§ÛŒ Ù‡ÙØªÚ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ú¯Ø²Ø§Ø±Ø´ Ú©Ù„ÛŒ
        schedule.every().monday.at("08:00").do(self.generate_weekly_report)

        logger.info("â° Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ Ú©Ø§Ø±Ù‡Ø§ ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯:")
        logger.info("  - Ù‡Ø± 30 Ø¯Ù‚ÛŒÙ‚Ù‡: Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§")
        logger.info("  - Ù‡Ø± Ø³Ø§Ø¹Øª: Ø¨Ø±Ø±Ø³ÛŒ Ø³Ù„Ø§Ù…Øª Ø³ÛŒØ³ØªÙ…")
        logger.info("  - Ù‡Ø± Ø±ÙˆØ² Ø³Ø§Ø¹Øª 2 ØµØ¨Ø­: ØªÙ…ÛŒØ²Ú©Ø§Ø±ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ")
        logger.info("  - Ù‡Ø± Ø¯ÙˆØ´Ù†Ø¨Ù‡ Ø³Ø§Ø¹Øª 8 ØµØ¨Ø­: Ú¯Ø²Ø§Ø±Ø´ Ù‡ÙØªÚ¯ÛŒ")

    def run_scheduled_job(self):
        """Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ø± Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯Ù‡"""
        if not self.is_running:
            self.is_running = True
            try:
                # Ø§Ø¬Ø±Ø§ÛŒ async Ø¯Ø± loop Ø¬Ø¯ÛŒØ¯
                asyncio.run(self.run_collection_job())
            finally:
                self.is_running = False

    def run_health_check(self):
        """Ø¨Ø±Ø±Ø³ÛŒ Ø³Ù„Ø§Ù…Øª Ø³ÛŒØ³ØªÙ…"""
        logger.info("ğŸ¥ Ø¨Ø±Ø±Ø³ÛŒ Ø³Ù„Ø§Ù…Øª Ø³ÛŒØ³ØªÙ…...")

        # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¶Ø±ÙˆØ±ÛŒ
        required_dirs = ['subscriptions']
        for directory in required_dirs:
            if not os.path.exists(directory):
                os.makedirs(directory)
                logger.info(f"ğŸ“ Ù¾ÙˆØ´Ù‡ {directory} Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯")

        # Ø¨Ø±Ø±Ø³ÛŒ ÙØ¶Ø§ÛŒ Ø¯ÛŒØ³Ú©
        import shutil
        total, used, free = shutil.disk_usage('.')
        free_gb = free // (1024**3)

        if free_gb < 1:  # Ú©Ù…ØªØ± Ø§Ø² 1 Ú¯ÛŒÚ¯Ø§Ø¨Ø§ÛŒØª ÙØ¶Ø§ÛŒ Ø¢Ø²Ø§Ø¯
            logger.warning(f"âš ï¸ ÙØ¶Ø§ÛŒ Ø¯ÛŒØ³Ú© Ú©Ù…: {free_gb}GB Ø¨Ø§Ù‚ÛŒâ€ŒÙ…Ø§Ù†Ø¯Ù‡")
        else:
            logger.info(f"ğŸ’¾ ÙØ¶Ø§ÛŒ Ø¯ÛŒØ³Ú©: {free_gb}GB Ø¢Ø²Ø§Ø¯")

    def cleanup_old_files(self):
        """ØªÙ…ÛŒØ²Ú©Ø§Ø±ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ"""
        logger.info("ğŸ§¹ Ø´Ø±ÙˆØ¹ ØªÙ…ÛŒØ²Ú©Ø§Ø±ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ...")

        import glob
        from datetime import datetime, timedelta

        # Ø­Ø°Ù Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ (Ø¨ÛŒØ´ Ø§Ø² 7 Ø±ÙˆØ²)
        old_reports = glob.glob('subscriptions/report_*.json')
        cutoff_date = datetime.now() - timedelta(days=7)

        deleted_count = 0
        for report_file in old_reports:
            try:
                file_time = datetime.fromtimestamp(
                    os.path.getctime(report_file))
                if file_time < cutoff_date:
                    os.remove(report_file)
                    deleted_count += 1
                    logger.info(f"ğŸ—‘ï¸ Ø­Ø°Ù Ú¯Ø²Ø§Ø±Ø´ Ù‚Ø¯ÛŒÙ…ÛŒ: {report_file}")
            except Exception as e:
                logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø­Ø°Ù {report_file}: {e}")

        logger.info(f"âœ… ØªÙ…ÛŒØ²Ú©Ø§Ø±ÛŒ Ú©Ø§Ù…Ù„ Ø´Ø¯ - {deleted_count} ÙØ§ÛŒÙ„ Ø­Ø°Ù Ø´Ø¯")

    def generate_weekly_report(self):
        """ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ù‡ÙØªÚ¯ÛŒ"""
        logger.info("ğŸ“Š ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ù‡ÙØªÚ¯ÛŒ...")

        try:
            # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¢Ù…Ø§Ø± Ù‡ÙØªÙ‡
            weekly_stats = {
                'period': 'Ù‡ÙØªÙ‡ Ú¯Ø°Ø´ØªÙ‡',
                'generated_at': datetime.now().isoformat(),
                'automation_stats': self.stats,
                'summary': {
                    'total_runs': self.stats['total_runs'],
                    'success_rate': f"{(self.stats['successful_runs'] / max(self.stats['total_runs'], 1)) * 100:.1f}%",
                    'average_configs_per_run': 0  # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ø² Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ÛŒ Ø±ÙˆØ²Ø§Ù†Ù‡
                }
            }

            # Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´ Ù‡ÙØªÚ¯ÛŒ
            os.makedirs('subscriptions', exist_ok=True)
            weekly_filename = f'subscriptions/weekly_report_{datetime.now().strftime("%Y%m%d")}.json'

            with open(weekly_filename, 'w', encoding='utf-8') as f:
                json.dump(weekly_stats, f, ensure_ascii=False, indent=2)

            logger.info(f"ğŸ“ˆ Ú¯Ø²Ø§Ø±Ø´ Ù‡ÙØªÚ¯ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {weekly_filename}")

        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ù‡ÙØªÚ¯ÛŒ: {e}")

    def start_automation(self):
        """Ø´Ø±ÙˆØ¹ Ø§ØªÙˆÙ…Ø§Ø³ÛŒÙˆÙ†"""
        logger.info("ğŸš€ Ø´Ø±ÙˆØ¹ Ø³ÛŒØ³ØªÙ… Ø§ØªÙˆÙ…Ø§Ø³ÛŒÙˆÙ†...")

        # ØªÙ†Ø¸ÛŒÙ… Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ
        self.setup_schedule()

        # Ø§Ø¬Ø±Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ‡
        logger.info("ğŸ”„ Ø§Ø¬Ø±Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ‡...")
        self.run_scheduled_job()

        # Ø­Ù„Ù‚Ù‡ Ø§ØµÙ„ÛŒ
        try:
            while True:
                schedule.run_pending()
                time.sleep(60)  # Ø¨Ø±Ø±Ø³ÛŒ Ù‡Ø± Ø¯Ù‚ÛŒÙ‚Ù‡

        except KeyboardInterrupt:
            logger.info("â¹ï¸ ØªÙˆÙ‚Ù Ø³ÛŒØ³ØªÙ… Ø§ØªÙˆÙ…Ø§Ø³ÛŒÙˆÙ†...")
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ Ø¯Ø± Ø§ØªÙˆÙ…Ø§Ø³ÛŒÙˆÙ†: {e}")
        finally:
            self.save_stats()
            logger.info("ğŸ’¾ Ø¢Ù…Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")

    def run_once(self):
        """Ø§Ø¬Ø±Ø§ÛŒ ÛŒÚ©Ø¨Ø§Ø±Ù‡ (Ø¨Ø¯ÙˆÙ† Ø§ØªÙˆÙ…Ø§Ø³ÛŒÙˆÙ†)"""
        logger.info("ğŸ”„ Ø§Ø¬Ø±Ø§ÛŒ ÛŒÚ©Ø¨Ø§Ø±Ù‡ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§...")

        try:
            asyncio.run(self.run_collection_job())
            logger.info("âœ… Ø§Ø¬Ø±Ø§ÛŒ ÛŒÚ©Ø¨Ø§Ø±Ù‡ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯")
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ ÛŒÚ©Ø¨Ø§Ø±Ù‡: {e}")


def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ"""
    import argparse

    parser = argparse.ArgumentParser(
        description='Ø³ÛŒØ³ØªÙ… Ø§ØªÙˆÙ…Ø§Ø³ÛŒÙˆÙ† V2Ray Config Collector')
    parser.add_argument('--mode', choices=['auto', 'once'], default='auto',
                        help='Ø­Ø§Ù„Øª Ø§Ø¬Ø±Ø§: auto (Ø®ÙˆØ¯Ú©Ø§Ø±) ÛŒØ§ once (ÛŒÚ©Ø¨Ø§Ø±Ù‡)')
    parser.add_argument('--interval', type=int, default=30,
                        help='ÙØ§ØµÙ„Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ø§Ø¬Ø±Ø§ Ø¨Ù‡ Ø¯Ù‚ÛŒÙ‚Ù‡ (ÙÙ‚Ø· Ø¯Ø± Ø­Ø§Ù„Øª auto)')

    args = parser.parse_args()

    automation = AutomationManager()

    if args.mode == 'once':
        automation.run_once()
    else:
        # ØªÙ†Ø¸ÛŒÙ… ÙØ§ØµÙ„Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ø³ÙØ§Ø±Ø´ÛŒ
        if args.interval != 30:
            schedule.every(args.interval).minutes.do(
                automation.run_scheduled_job)

        automation.start_automation()


if __name__ == "__main__":
    main()
